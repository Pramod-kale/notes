//^ Big O Notation, what is it?
//@ Big O Notation is a way to measure how long it takes for an algorithm to run.
//@ simplified analysis of an algorithms effeciency
//@ gives us the complexity in terms of input size
//@ it is machine independant
//@ used to analyse time and space

//^ type of measurement
//@ worst case
//@ best case
//@ average case
//! when working we typically look at worst case

//^ general rules
//@ it ignores constants
//@ terms which dominate others
        //$ O1 < Ologn < On < Onlogn < On^2 < O2n <On!

